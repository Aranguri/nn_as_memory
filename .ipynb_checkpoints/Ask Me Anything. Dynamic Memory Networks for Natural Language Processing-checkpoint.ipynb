{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost every NLP task can be casted into QA.\n",
    "\n",
    "# Modules\n",
    "Module overview\n",
    "Input: in: input sentence. out: distributed representation\n",
    "Question: in: \n",
    "Memory: \n",
    "Answer: in: last memory. out: task answer\n",
    "\n",
    "## Input\n",
    "input = word_embeddings([w_1, w_2, ... , w_n])\n",
    "\n",
    "for x in input\n",
    "    h_t = GRU(x, h_{t-1})\n",
    "    \n",
    "output = h if sentences(input) == 1 else {h_i | h_i was hidden state after EOS}\n",
    "\n",
    "## Question\n",
    "question = word_embeddings([qw_1, qw_2, ..., qw_n]) #The word embedding is shared for input and quesiton.\n",
    "\n",
    "for x in question\n",
    "    h_t = GRU(x, h_{t-1})\n",
    "    \n",
    "return h_T\n",
    "\n",
    "## episodic memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h^i_t = gate(GRU(c_t, h^i_{t-1}), h^i_{t-1})$\n",
    "\n",
    "$e^i = h^i_{T_c}$\n",
    "\n",
    "$m^i = GRU(e^i, m^{i - 1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eat one by one the words so as to compute a single representation for all the data. We use the hidden state produced after eating all the words to update the memory.\n",
    "\n",
    "{what are the advantages of having the gate for $h_t^i$?}\n",
    "\n",
    "It's interesting that we are taking more into account the most recent sentences. \n",
    "\n",
    "#### rnns and gates (for elements)\n",
    "It seems that rnns and gates are related. We could see the gate as a rnn simplification. \n",
    "\n",
    "$h_t = gate(x, h_{t-1})$\n",
    "\n",
    "$h_t = rnn(x, h_{t-1})$\n",
    "\n",
    "What's the difference in the computation?\n",
    "\n",
    "$gate(x, h_{t-1}) = g \\odot x + (1 - g) \\odot h_{t-1}$\n",
    "\n",
    "$rnn(x, h_{t-1}) = act\\_fun(Ux + Vh_{t-1} + b)$\n",
    "\n",
    "How do we get from a gate to a rnn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hadamard product $g \\odot x$ could be seen as a diagonal matrix multiplying x. That is, $diag(g)x = g \\odot x$ (where diag(k) constructs a matrix with the values of the vector k in the diagonal entries.) In other words, $g \\odot x$ is a specific case of $Ux$ where U is diagonal. (Geometrically, a diagonal matrix means we are only stretching the dimensions of the vector without any rotation.)\n",
    "\n",
    "Now, remember the origin of the parameter g in the gate. (not sure:) almost always, it comes from a sigmoid function. Thus, there we have another constraint!\n",
    "\n",
    "The fact that we have $diag(g)$ and $diag(1 - g)$ instead of $U$ and $V$ means we are constraining U and V to sum to the identity (and thus to have the same dimension.)\n",
    "\n",
    "Finally, if the activation function is the identity function and the bias vector is the zero vector, we arrive to the gate.\n",
    "\n",
    "It's interesting: one step of a rnn is a generalization of a gate. The rnn is much more powerful than the gate, but the specificity and simplicity of the gate could make it more useful for cases where we know that we need a gate. [1]\n",
    "\n",
    "{a gate doesn't care about the order of the inputs. but a rnn does. is there a version of the rnn where we don't care about the order. ie, where we have either two hidden states or two input states? Having two hidden states h_1 and h_2 is the same as having the hidden state h_3 = [h_1, h_2]. What does it mean to have only one hidden state without any input? Every iteration in the rnn is just a new layer! But with fixed weights over the different layers. is that useful? it's somethign like Wtanh(Wtanh(Wtanh(Wx)))}\n",
    "\n",
    "{try with grus}\n",
    "\n",
    "[1] Say we have the space of tools and the space of problems. We can think about a tool as covering part of the space with a mantle. Generally, the more the mantle covers the thinner it is. We can also think about a problem as another mantle with a fixed height of 1. Now, the performance in solving the problem is determined by the integral of the tool mantle that coincides with the problem mantle (in other words, we apply a convolution between the two mantles.) The thicker the mantle the better. But we need the mantle to be covering the subspace of the problem. Thus, a more specific rule applied to the right problems will yield better results than a general tool.\n",
    "\n",
    "For instance, logistic regression has a specific case an algorithm called gaussian mixture model (gmm) which tries to fit gaussian distributions to the data. If the data comes from gaussian distributions, then gmm will perform better than logistic regression. However, if the data comes from other distributons, then logistic regression will perform much better than gmm. {how big is that difference?}\n",
    "\n",
    "Say we try to come up with a completely general algorithm, one which mantle is huge {is it infinite?} and covers all the tasks. Then, the No Free Lunch theorem tells us that this mantle will be zero. That is, the performance of this general algorithm will be the same as giving a random answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Similarity measure\n",
    "First, we define a feature vector. Notice we only care about the interactions that include c; the interactions between m and q are the same throughout the stored facts c.\n",
    "\n",
    "z(c, m, q) = [c, m, q, c ◦ q, c ◦ m, |c − q|, |c − m|, c^TWq, c^TWm]\n",
    "\n",
    "{try the generalized_hadamard(x, y) = hadamard(x, affine(y)) = x ◦ Ay}\n",
    "{try removing values from the feature vector. does the performance change?}\n",
    "{how will be the interaction between three things?}\n",
    "\n",
    "G(c, m, q) = nn(z(c, m, q))\n",
    "\n",
    "with nn(x) = sigmoid(wb(tanh(wb(x))))\n",
    "\n",
    "What's e_i for?\n",
    "At each timestep, we want to update the memory based on the facts that are useful. To detect whether a fact is useful, we compute the similarity between the fact and the previous memory and between the fact and the question.\n",
    "\n",
    "The way we compute e^i puts an emphasis on the last part of the sentence. (not sure:) it seems another valid approach to compute e^i would be to do attention(c, weights=g). In general, I don't know why we would like to have an emphasis on the last part of the sentence. At least, it seems an useful prior/assumption for tasks that asks about the actual state of the world. So if we changed something from place a hundred times, we only care about the last change. I wonder if this generalizes well to all types of tasks.\n",
    "\n",
    "{it doesn't seem that good that the only way to retrieve a memory that is useful for a representation of the task is by looking at the similarity between the memory and the representation of the task. it might be the case that key-value memory networks solve this problem}\n",
    "\n",
    "recurrent vs recursive: https://stats.stackexchange.com/questions/153599/recurrent-vs-recursive-neural-networks-which-is-better-for-nlp\n",
    "how does a recurrent nn looks like with circles and arrows?\n",
    "\n",
    "## answer module\n",
    "The last memory contains the answer to the question. But the answer to the question could be a bunch of words. So we take a GRU that can decouple information from the hidden state and produce the words that answer the question. \n",
    "\n",
    "$$\n",
    "a_0 = m^{T_M} \\\\\n",
    "a_t = GRU([q, y_{t-1}], a_{t-1}) \\\\\n",
    "y_t = softmax(Wa_t) \\\\\n",
    "$$\n",
    "{why do we input the question q at each timestep and we don't do that with the memory m? one answer is that we want to remove everything that we already processed from the hidden state. however, the same applies to the question.}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
