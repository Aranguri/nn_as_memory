{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANNs\n",
    "## Notes\n",
    "### Essays\n",
    "* Introduction to memory networks\n",
    "* Long-term memories\n",
    "\n",
    "### Papers\n",
    "* Ask Me Anything. Dynamic Memory Networks for Natural Language Processing\n",
    "* Differentiable Neural Computer {incomplete}\n",
    "* Memory Networks {incomplete}\n",
    "* Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes {incomplete}\n",
    "* Neural Turing Machine {on sheet of paper}\n",
    "* MAC Network {on sheet of paper}\n",
    "* Attention and Augmented Recurrent Neural Networks\n",
    "\n",
    "\n",
    "## Future things\n",
    "create a roadmap\n",
    "what if a nn is connected to the internet?\n",
    "are computers taking advantage of good assumptions we can make on the type of data humans have? does this even make sense?\n",
    "is there a way of using the kernel trick? we could think the memory at time t as a linear combination of the input and the memory at previous timesteps\n",
    "We need a long-term memory.\n",
    "* we can't read all the memories at once. So we need to try with something like a tree (hierarchical structure), a graph (relational structure), hard attention, or something I call recursive hopfield net. [3]\n",
    "* we need to know what we are going to store.\n",
    "* we need to model long-term dependnces, but backpropagation through large amounts of time doesn't work that well. We need to explore this more.\n",
    "* we can connect it to the internet\n",
    "* we can let the nn communicate with each other\n",
    "* we should also have a short-term memory, and decide what goes from there to the long-term (we can use: the less you attend something, the more it disappears from your short-term memory. Also, the more you attend something, the higher are the chances of saving it into your memory.)\n",
    "Task: given a description in wikimedia, it has to generate the label (or vice versa.)\n",
    "\n",
    "is the softargmax also a gate?\n",
    "next steps\n",
    "* look for a dataset where it makes sense to have a huge memory\n",
    "* fix memory not changing in the same run\n",
    "\n",
    "* Problems: I give you eight real numbers. You have only four slots to store real numbers. How precise can your answer be? Can you recover the full four numbers?\n",
    "* the output of the RNN_{MG} has memory_size. What if it only has size 1. ie, what if we write one memory instead of the whole thing.\n",
    "* {consider that in my implementation each input receives only one iteration of processing through the unit. it would be interesting to have as many processing units as the neural nets thinks is good}\n",
    "\n",
    "* https://github.com/unixpickle/sgdstore\n",
    "\n",
    "lagrange multipliers\n",
    "\n",
    "### Papers\n",
    "* Neural RAM: https://arxiv.org/pdf/1511.06392.pdf\n",
    "* Neural gpus: https://arxiv.org/abs/1511.08228\n",
    "* MAC: https://arxiv.org/pdf/1803.03067.pdf\n",
    "* RL-NTM: https://arxiv.org/pdf/1505.00521.pdf\n",
    "* End to end memory networks: https://arxiv.org/pdf/1503.08895.pdf\n",
    "* One-shot learning: https://arxiv.org/pdf/1605.06065v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
