{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Networks\n",
    "Summary: transform the input. store it in the memory. retrieve memories that are r\n",
    "\n",
    "Four components:\n",
    "* I: transform input. in: input. out: trans(input)\n",
    "* G: store memories. in: trans(input), m_i and all memories. out: updated m_i\n",
    "* O: retrieve memories. in: trans(input), all memories. out: trans(output)  \n",
    "* R: untransform output. in: trans(output). out: output\n",
    "\n",
    "## G comopnent\n",
    "We store the input in the memories. We can store by categories, and instead of storing everything in a new slot, we can update the previous one in \"real time.\"\n",
    "\n",
    "An efficient way to forget is to remove the memory with the least expected utility.\n",
    "\n",
    "## O component\n",
    "We want to retrieve the most useful n memories. We think of usefulness as how similar is the memory to the input and the previous retrieved memories.\n",
    "\n",
    "$m^n = argmax_{m_i} s_O([input, m^1, ..., m^{n-1}], m_i)$\n",
    "\n",
    "Another option could have been taking the first m_i's that maximize s_O. But conditioning on the already-retrieved memories could give us a better context to continue retrieving memories. (In particular, we could refine the memory a little bit in each time step. For instance, we can start with a blurred memory of a six and we retrieve memories that are more and more clean.)\n",
    "\n",
    "## R component\n",
    "Given the selected memories, now we want to select the word that is most similar to the input and the memories. \n",
    "\n",
    "$r = argmax_{w \\in W} s_R([input, m^1, ..., m^k], w)$\n",
    "\n",
    "A standard algorithm without memory would skip component O and wouldn't condition r on the memories. The hyphothesis that MemNNs are based on is that we only care about so many memories. Thus, having separated steps for selecting memories and using them could be useful.\n",
    "\n",
    "## Score function $s_O$ and $s_R$\n",
    "Before computing the similarity function of the two inputs, we want to map the inputs to a space better suited for computing the similarity. Thus, we define a transformation \n",
    "\n",
    "$\\tilde x = transform(x) = affine(bag\\_of\\_words(x)) = U bag\\_of\\_words(x).$ [#]\n",
    "\n",
    "We then compute the similarity as a dot product between the transformations.\n",
    "\n",
    "$similarity(transform(x), transform(x)) = transform(x)^Ttransform(x)$ \n",
    "\n",
    "{in the paper, they say the bag_of_words(x) function has three dimensions: one to use when the input is y and two to use when the input is x. why don't they use different functions for bag_of_words? I understand that for bag_of_words(x) you need at least D = 2|W| for you have words that come from the input x and from the memory, so we count the words appearance differently. But why don't we have $bag\\_of\\_words_x \\in R^{2|W|}$ and $bag\\_of\\_words_y \\in R^{|W|}$}\n",
    "\n",
    "## Training\n",
    "We have supervised information about the sentences that should have been selected and the response. We use the margin ranking loss that maximizes the distance between the score given to the correct memory location and that given to all the other memory locations. We only care about maximizing the distance up to a certain threshold. For instance, if the distance between the correct memory and an incorrect memory is greater than the threshold, then  the loss remains the same if we multiply that distance by 3.\n",
    "\n",
    "Randomized algorithms help: instead of going through all the other memory locations, we only compute the loss for a subset of the locations.\n",
    "\n",
    "In the following loss we assume we retrieve only two memories in the G step. $m_1$ and $m_2$ are the labels for the memory and $r$ is the label for the response. We want to minimize the following loss.\n",
    "\n",
    "$$\n",
    "L(U_O, U_R) = \\sum_{\\bar m \\neq m_1}max(0, \\gamma - [s_O(x, m_1) - s_O(x, \\bar m)]) + \\\\\n",
    "\\sum_{\\bar m \\neq m_2}max(0, \\gamma - [s_O([x, m_1], m_2) - s_O([x, m_1], \\bar m)]) + \\\\\n",
    "\\sum_{\\bar w \\neq r}max(0, \\gamma - [s_R([x, m_1, m_2], r) - s_R([x, m_1, m_2], \\bar w)])\\\\\n",
    "$$\n",
    "\n",
    "## Segmentation\n",
    "If the input arrives as words instead of sentences, the model needs to learn where to start a new sentence. \n",
    "\n",
    "To do this, we first transform x to an embedding space. Then, we use a linear classifier that based on the sequence of words, it classifies whether it corresponds to a complete sentence or not. If it corresponds, we store that sentence and we start a new sentence with the next word. If it doesn't corresond, we add to the sentence the next word. \n",
    "\n",
    "$linear\\_classifier(transform(x))$\n",
    "\n",
    "## Efficiency\n",
    "### Hashing\n",
    "Requirements:\n",
    "* we have a function f that searches on average on time O(1)\n",
    "* we have a list of vectors A and a vector v\n",
    "* we want to search the vector in A that is most similar to v\n",
    "* we don't want to compare v to every vector in A\n",
    "\n",
    "Process:\n",
    "* We compare v to A_i iff f(v) == f(A_i) \n",
    "\n",
    "An example of this is A being a list of real numbers, f being the floor function, and v being another real number. Thus, we don't compare v to every number in A. Instead, we compute the floor function of v, and compare the result to three buckets: the buckets where $f(v) == f(A_i)$ and the buckets where $f(v) \\pm 1 == f(A_i).$ Notice that if we draw 100 numbers from an uniform distribution between 0 and 10, the complexity is the same as drawing 10n numbers from 0 to n (with n being any natural.) This happens because the complexity of indexing in an array is constant[#].\n",
    "\n",
    "### Implementation\n",
    "#### First option\n",
    "We select the memories that share at least one word with the input.\n",
    "We do this by using\n",
    "* First, using a hash function that maps every word to a single bucket.\n",
    "* Then, we hash the input and every memory. (Notice that we need to compute the hash for the memory only once.) This hash function will return as many buckets as there are words in the sentence.\n",
    "* Finally, we only compare the input to those memories that share at least one word with the sentence. \n",
    "\n",
    "#### Second option\n",
    "Instead of having one bucket for every word, we have k buckets where in each bucket we have similar words. We can obtain this by running k-means. We then consider the memories that share buckets with the input. \n",
    "\n",
    "{section 3.4 onwards https://arxiv.org/pdf/1410.3916.pdf}\n",
    "\n",
    "## Other concepts\n",
    "bag-of-words: a representation of a document where instead of storing the whole document, we store a dictionary with the keys being words and the values being the number of times the words appear in the document.\n",
    "\n",
    "array index is O(1): We know that all the entries of an array are consecutively stored in memory. We also know the datatype of the array (and hence we know the memory size of every entry.) So, if we want to retrieve/store an entry in an array, we do it in O(1) by going to the memory address of first entry, and adding n * memory_size (with n being the position of the value in the array.) \n",
    "\n",
    "## Notes\n",
    "[1] I think that whenever there is something that we can do efficiently, we are taking advantage of some assumption. In this case, we are exploiting the fact that we stored the memories in an organized and predictable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Key-value memories\n",
    "## First step\n",
    "Key hashing: we preselect all the memories which keys share at least one word (except common words like the, a, etc) with the question.\n",
    "\n",
    "Key addressing:\n",
    "* softmax(affine(keys), weights=affine(q))\n",
    "* affine(k) = A\\phi_K(k)\n",
    "* affine(q) = A\\phi_Q(k)\n",
    "\n",
    "{why do they use the same transformation A for both?}\n",
    "\n",
    "Reading: we read each value vector by the weight given by the similarity between the keys and the question.\n",
    "* o = sum_\n",
    "\n",
    "## Next steps\n",
    "q_{i+1} = R_{i+1} (o + q_i)\n",
    "We don't do key hashing\n",
    "affine(q) = q_{i+1}\n",
    "\n",
    "## Last step\n",
    "We want the word that is most similar to the last memory.\n",
    "\n",
    "* argmax_i(similarity(q_H, affine(y_i)))\n",
    "* similarity(x, y) = x^TAy\n",
    "* affine(y) = \\phi(y)\n",
    "\n",
    "{this seems to be a good place for the kernel trick, for everything is a linear combination of the original memories}\n",
    "\n",
    "## Other details\n",
    "If A relation B, we store both (A, relation, B) and (B, relation.inv, A)\n",
    "\n",
    "Window level: we store as a key a window of n words with the center word being an entity. The value is just the entity.\n",
    "\n",
    "Window + key encoding: we encode the center word and the value with another dictionary, so as to know whether an entity appears in the center or not.\n",
    "\n",
    "## Results\n",
    "KB > raw wikipedia > IE\n",
    "It's encouraging that raw wikipedia > IE, but the difference between KB and Raw wikipedia is large (94% to 76%). But most of that loss (94% to 83%) is just due to representing the information in sentences instead of triples. (For insance, there could be one-five words representing the relationship now. [Movie came out in year, instead of movie come_out_in year])\n",
    "\n",
    "## Other concepts\n",
    "Data sparsity issues: in NLP, most of the sentences we see are new, and new words appear often. Also, there could be a lot of redundancy. This is (in part) why latent semantic analysis, principal component analysis, and word embeddings make sense.\n",
    "\n",
    "coreference: Eg _Mary_ was going to the library because _she_ wanted to play (Mary and she are coreferring each other.)\n",
    "\n",
    "Mean average precision: \n",
    "\n",
    "\n",
    "# Neural module networks\n",
    "We can draw lessons from formal systems. Eg, conjunction, disjunction, does it exist?\n",
    "\n",
    "Why is this required? We are performing a particular transformation depending on the image and text we are using. In other works (Key-Value mem nn), the transformation is fixed. That is, we have a matrix A and a feature function \\phi that is the same for every question/memory. Here, depending on the question {and on the image/memory}, we use some specific program. \n",
    "\n",
    "We model two probabilities. \n",
    "* p(z|x): given the question x, what's the probability of this module z.\n",
    "* p_z(y|w): given that we are using module z and we're in the world w, what's the probability of this answer y\n",
    "\n",
    "* [[ z ]]_w: output of layout z given module w.\n",
    "* p_z(y|w) = ([[ z ]]_w)_y #We assume that the output of the layour z are a prob distribution over all the answers y. \n",
    "\n",
    "\n",
    "## Modules\n",
    "### Lookup (-> attention)\n",
    "lookup[i] = eye[i]\n",
    "\n",
    "### Find\n",
    "softargmax(similarity(affine(v_i), affine(W)))\n",
    "similarity(x, y) = x + y + bias\n",
    "affine(v_i) = Bv_i\n",
    "affine(W) = CW\n",
    "\n",
    "### Relate\n",
    "{attention -> attention sounds interesting. say we have one vector and an array of vectors. If we use attention, we are going to generate a prob distribution over the array of vectors. Now, given the prob distribution and the vector, can you recover the array of vectors. Also, using the prob distribution and the array of vectors, can you recover the vectors?}\n",
    "\n",
    "Ex: \n",
    "A = [[1, 0], [1/sqrt(2), 1/sqrt(2)], [0, 1]]\n",
    "v = [3/sqrt(5), 4/sqrt(5)]\n",
    "p = [3/sqrt(5), \n",
    "\n",
    "we have unit vectors, so a^2+b^2 = 1\n",
    "Also, a*x + b*y = c\n",
    "So given x and y, we can recover a and b\n",
    "\n",
    "K-Now, say you you have an n-dimensional vector, and you attend over k n-dimensional vectors (with k \\geq n.) Now, if the k vectors we attend over are different, then we have k systems of equation relating the n variables. We take n of those equations and we can solve exactly for the values of the key vector. (if we are using unit vectors, then we only need n-1 equations.) {how can we approximate the answer if we don't have that many vectors in memory.}\n",
    "\n",
    "{what about the reverse side}\n",
    "\n",
    "Elements: the the magniude of the sum of two unit vectors computes its similarity. \n",
    "a = [1/s(4), -s(2)/s(4), 1/s(4)]\n",
    "b = [-s(3)/s(5), 0, s(2)/s(5)]\n",
    "a+b = [-0.27459667, -0.70710678,  1.13245553]\n",
    "||a+b||=1.36\n",
    "a+a=[ 1.        , -1.41421356,  1.        ]\n",
    "||a+b||=2\n",
    "c = [1/s(2), 1/s(2)]\n",
    "d = [-1/s(2), -1/s(2)]\n",
    "||c+d|| = 0\n",
    "\n",
    "\n",
    "Continue: modules on page 4. Also, try to avoid continue reading papers and focus on deep understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a roadmap\n",
    "what if a nn is connected to the internet?\n",
    "are computers taking advantage of good assumptions we can make on the type of data humans have? does this even make sense?\n",
    "is there a way of using the kernel trick? we could think the memory at time t as a linear combination of the input and the memory at previous timesteps\n",
    "\n",
    "\n",
    "\n",
    "We need a long-term memory.\n",
    "* we can't read all the memories at once. So we need to try with something like a tree (hierarchical structure), a graph (relational structure), hard attention, or something I call recursive hopfield net. [3]\n",
    "* we need to know what we are going to store.\n",
    "* we need to model long-term dependnces, but backpropagation through large amounts of time doesn't work that well. We need to explore this more.\n",
    "* we can connect it to the internet\n",
    "* we can let the nn communicate with each other\n",
    "* we should also have a short-term memory, and decide what goes from there to the long-term (we can use: the less you attend something, the more it disappears from your short-term memory. Also, the more you attend something, the higher are the chances of saving it into your memory.)\n",
    "Task: given a description in wikimedia, it has to generate the label (or vice versa.)\n",
    "\n",
    "is the softargmax also a gate?\n",
    "next steps\n",
    "* look for a dataset where it makes sense to have a huge memory\n",
    "* fix memory not changing in the same run\n",
    "\n",
    "* Problems: I give you eight real numbers. You have only four slots to store real numbers. How precise can your answer be? Can you recover the full four numbers?\n",
    "* the output of the RNN_{MG} has memory_size. What if it only has size 1. ie, what if we write one memory instead of the whole thing.\n",
    "* {consider that in my implementation each input receives only one iteration of processing through the unit. it would be interesting to have as many processing units as the neural nets thinks is good}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
